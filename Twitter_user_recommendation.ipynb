{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter user recommendation\n",
    "## Models and algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "stdin, stdout, stderr = sys.stdin, sys.stdout, sys.stderr\n",
    "reload(sys)\n",
    "sys.stdin, sys.stdout, sys.stderr = stdin, stdout, stderr\n",
    "sys.setdefaultencoding('utf8')\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import simplejson as json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from afinn import Afinn\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, accuracy_score\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import spatial\n",
    "import json\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out bots and boring users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean and tokenize tweets\n",
    "def rm_html_tags(str): ## discard tags\n",
    "    html_prog = re.compile(r'<[^>]+>',re.S)\n",
    "    return html_prog.sub('', str) ## re.sub(pattern, repl, string)\n",
    "\n",
    "def rm_html_escape_characters(str):\n",
    "    pattern_str = r'&quot;|&amp;|&lt;|&gt;|&nbsp;|&#34;|&#38;|&#60;|&#62;|&#160;|&#20284;|&#30524;|&#26684|&#43;|&#20540|&#23612;'\n",
    "    escape_characters_prog = re.compile(pattern_str, re.S)\n",
    "    return escape_characters_prog.sub('', str)\n",
    "\n",
    "def rm_at_user(str):\n",
    "    return re.sub(r'@[a-zA-Z_0-9]*', '', str)\n",
    "\n",
    "def rm_url(str):\n",
    "    return re.sub(r'http[s]?:[/+]?[a-zA-Z0-9_\\.\\/]*', '', str)\n",
    "\n",
    "def rm_repeat_chars(str):\n",
    "    return re.sub(r'(.)(\\1){2,}', r'\\1\\1', str)\n",
    "\n",
    "def rm_hashtag_symbol(str):\n",
    "    return re.sub(r'#', '', str)\n",
    "\n",
    "def replace_emoticon(emoticon_dict, str):\n",
    "    for k, v in emoticon_dict.items():\n",
    "        str = str.replace(k, v)\n",
    "    return str\n",
    "\n",
    "def rm_time(str):\n",
    "    return re.sub(r'[0-9][0-9]:[0-9][0-9]', '', str)\n",
    "\n",
    "def rm_punctuation(current_tweet):\n",
    "    return re.sub(r'[^\\w\\s]','',current_tweet)\n",
    "\n",
    "\n",
    "def pre_process(str0):\n",
    "    # do not change the preprocessing order only if you know what you're doing \n",
    "    str0 = str0.lower()\n",
    "    str0 = rm_url(str0)        \n",
    "    str0 = rm_at_user(str0) ## think of what you can extract from user's name?       \n",
    "    str0 = rm_repeat_chars(str0) ## the num of repeats may count?\n",
    "    str0 = rm_hashtag_symbol(str0) ## hashtags is important!      \n",
    "    str0 = rm_time(str0)        \n",
    "    str0 = rm_punctuation(str0)\n",
    "        \n",
    "    str0 = nltk.tokenize.word_tokenize(str0) \n",
    "    str0 = [s for s in str0 if s not in stops]\n",
    "\n",
    "    return str0\n",
    "\n",
    "## generate list containing all the stopwords\n",
    "stops = []\n",
    "path = './stopwords'\n",
    "for filename in os.listdir(path):\n",
    "    with open(os.path.join(path, filename).encode('utf-8')) as f:\n",
    "        line = f.readline()\n",
    "        stop = [i.strip() for i in line.split(',')]\n",
    "        stops = stops + stop\n",
    "\n",
    "stops = stops + list(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## detect url and at\n",
    "def detect_url(str):\n",
    "    prog = re.compile(r'http[s]?:[/+]?[a-zA-Z0-9_\\.\\/]*')\n",
    "    result = prog.search(str)\n",
    "    if result:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def detect_at_user(str):\n",
    "    prog = re.compile(r'@[a-zA-Z_0-9]*')\n",
    "    result = prog.search(str)\n",
    "    if result:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get user tweets stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start loading and process data...\n"
     ]
    }
   ],
   "source": [
    "data_dir = './users with tweets before filtering'  ##Setting your own file path here.\n",
    "tweets = 'new_tweets_part1.txt'\n",
    "\n",
    "# stops = []\n",
    "# path = './stopwords'\n",
    "# for filename in os.listdir(path):\n",
    "#     with open(os.path.join(path, filename).encode('utf-8')) as f:\n",
    "#         line = f.readline()\n",
    "#         stop = [i.strip() for i in line.split(',')]\n",
    "#         stops = stops + stop\n",
    "\n",
    "# stops = stops + list(stopwords.words('english'))\n",
    "stops = list(stopwords.words('english'))\n",
    "\n",
    "user_info = []\n",
    "print('start loading and process data...')\n",
    "id_url_count_list = []\n",
    "id_at_count_list = []\n",
    "id_len_list = []\n",
    "with open(os.path.join(data_dir, tweets).encode('utf-8'),'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        tweet_obj = line.strip().split('::::')\n",
    "        uid = tweet_obj[0]\n",
    "        tweet = tweet_obj[1]\n",
    "        created_at = tweet_obj[2]\n",
    "        quote_count = tweet_obj[3]\n",
    "        retweet_count = tweet_obj[4]\n",
    "        favorite_count = tweet_obj[5]\n",
    "        favorited = tweet_obj[6]\n",
    "        retweeted = tweet_obj[7]\n",
    "        entities = tweet_obj[8]\n",
    "\n",
    "        id_url_count = uid+','+str(detect_url(tweet))\n",
    "        id_url_count_list.append(id_url_count)\n",
    "        \n",
    "        id_at_count = uid+','+str(detect_at_user(tweet))\n",
    "        id_at_count_list.append(id_at_count)\n",
    "        \n",
    "        cleaned_text = pre_process(tweet)\n",
    "\n",
    "        id_len_tweet = uid+','+str(len(cleaned_text))\n",
    "        id_len_list.append(id_len_tweet)\n",
    "\n",
    "fout = open(os.path.join(data_dir, 'tweets_url_count.txt').encode('utf-8'), 'w')\n",
    "for i in id_url_count_list:\n",
    "    fout.write('%s\\n' %i)\n",
    "fout.close()\n",
    "\n",
    "fout = open(os.path.join(data_dir, 'tweets_at_count.txt').encode('utf-8'), 'w')\n",
    "for i in id_at_count_list:\n",
    "    fout.write('%s\\n' %i)\n",
    "fout.close()\n",
    "\n",
    "fout = open(os.path.join(data_dir, 'tweets_len.txt').encode('utf-8'), 'w')\n",
    "for i in id_len_list:\n",
    "    fout.write('%s\\n' %i)\n",
    "fout.close()\n",
    "print('Process completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "286\n"
     ]
    }
   ],
   "source": [
    "## filter out suspecious users\n",
    "df = pd.read_csv('./users with tweets before filtering/tweets_url_count.txt',header=None,names=['id','url_ratio'])\n",
    "df_grouped = df.groupby('id', as_index=False).count()\n",
    "df_grouped = df_grouped.loc[df_grouped['url_ratio']>80] # users with more than 80 tweets\n",
    "after_count_list = df_grouped['id'].tolist()\n",
    "\n",
    "df = pd.read_csv('./users with tweets before filtering/tweets_url_count.txt',header=None,names=['id','url_ratio'])\n",
    "df_grouped = df.groupby('id', as_index=False).mean()\n",
    "df_grouped = df_grouped.loc[df_grouped['url_ratio']<0.5] # users with less than 0.5 url ratio\n",
    "after_url_list = df_grouped['id'].tolist()\n",
    "\n",
    "df = pd.read_csv('./users with tweets before filtering/tweets_at_count.txt',header=None,names=['id','at_ratio'])\n",
    "df_grouped = df.groupby('id', as_index=False).mean()\n",
    "df_grouped = df_grouped.loc[df_grouped['at_ratio']<0.8] # users with less than 0.8 at ratio\n",
    "after_at_list = df_grouped['id'].tolist()\n",
    "\n",
    "df = pd.read_csv('./users with tweets before filtering/tweets_len.txt',header=None,names=['id','word_count'])\n",
    "df_grouped = df.groupby('id', as_index=False).mean()\n",
    "df_grouped = df_grouped.loc[df_grouped['word_count']>7] # users with less than 7 average number of meaningful words \n",
    "after_wordcount_list = df_grouped['id'].tolist()\n",
    "\n",
    "\n",
    "set0 = set(after_url_list).intersection(after_at_list)\n",
    "set0 = set0.intersection(after_wordcount_list)\n",
    "set0 = set0.intersection(after_count_list)\n",
    "final_user_list = list(set0)\n",
    "\n",
    "print len(final_user_list)\n",
    "\n",
    "fout = open(os.path.join(data_dir, 'filtered_user_part1.txt').encode('utf-8'), 'w')\n",
    "for i in final_user_list:\n",
    "    i = str(i).replace('L','')\n",
    "    fout.write('%s\\n' %i)\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA: Tweets-based recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we train our model by all the tweets we have (from seed user and their friends and followers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './final data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 0th 10000 iteration\n",
      "The 1th 10000 iteration\n",
      "The 2th 10000 iteration\n",
      "The 3th 10000 iteration\n",
      "The 4th 10000 iteration\n",
      "The 5th 10000 iteration\n",
      "The 6th 10000 iteration\n",
      "The 7th 10000 iteration\n",
      "The 8th 10000 iteration\n",
      "The 9th 10000 iteration\n",
      "The 10th 10000 iteration\n",
      "The 11th 10000 iteration\n",
      "The 12th 10000 iteration\n",
      "The 13th 10000 iteration\n",
      "The 14th 10000 iteration\n",
      "The 15th 10000 iteration\n",
      "The 16th 10000 iteration\n",
      "The 17th 10000 iteration\n",
      "The 18th 10000 iteration\n",
      "The 19th 10000 iteration\n",
      "The 20th 10000 iteration\n",
      "The 21th 10000 iteration\n",
      "The 22th 10000 iteration\n",
      "The 23th 10000 iteration\n",
      "The 24th 10000 iteration\n",
      "The 25th 10000 iteration\n",
      "The 26th 10000 iteration\n",
      "The 27th 10000 iteration\n",
      "The 28th 10000 iteration\n",
      "The 29th 10000 iteration\n",
      "The 30th 10000 iteration\n",
      "The 31th 10000 iteration\n",
      "The 32th 10000 iteration\n",
      "The 33th 10000 iteration\n",
      "The 34th 10000 iteration\n",
      "The 35th 10000 iteration\n",
      "The 36th 10000 iteration\n",
      "The 37th 10000 iteration\n",
      "The 38th 10000 iteration\n",
      "The 0th 10000 iteration\n",
      "The 1th 10000 iteration\n",
      "The 2th 10000 iteration\n",
      "The 3th 10000 iteration\n",
      "The 4th 10000 iteration\n",
      "The 5th 10000 iteration\n",
      "The 6th 10000 iteration\n",
      "The 7th 10000 iteration\n",
      "The 8th 10000 iteration\n",
      "The 9th 10000 iteration\n",
      "The 10th 10000 iteration\n",
      "The 11th 10000 iteration\n",
      "The 12th 10000 iteration\n",
      "The 13th 10000 iteration\n",
      "The 14th 10000 iteration\n",
      "The 15th 10000 iteration\n",
      "The 16th 10000 iteration\n",
      "The 17th 10000 iteration\n",
      "The 18th 10000 iteration\n",
      "The 19th 10000 iteration\n",
      "The 20th 10000 iteration\n",
      "The 21th 10000 iteration\n",
      "The 22th 10000 iteration\n",
      "The 23th 10000 iteration\n",
      "The 24th 10000 iteration\n",
      "The 25th 10000 iteration\n",
      "The 26th 10000 iteration\n",
      "The 27th 10000 iteration\n",
      "The 28th 10000 iteration\n",
      "The 29th 10000 iteration\n",
      "The 0th 10000 iteration\n",
      "The 1th 10000 iteration\n",
      "The 2th 10000 iteration\n",
      "The 3th 10000 iteration\n",
      "The 4th 10000 iteration\n",
      "The 5th 10000 iteration\n",
      "The 6th 10000 iteration\n",
      "The 7th 10000 iteration\n",
      "The 8th 10000 iteration\n",
      "The 9th 10000 iteration\n",
      "The 10th 10000 iteration\n",
      "The 11th 10000 iteration\n",
      "The 12th 10000 iteration\n",
      "The 13th 10000 iteration\n",
      "The 14th 10000 iteration\n",
      "The 15th 10000 iteration\n",
      "The 16th 10000 iteration\n",
      "The 17th 10000 iteration\n",
      "The 18th 10000 iteration\n",
      "The 19th 10000 iteration\n",
      "The 20th 10000 iteration\n",
      "The 21th 10000 iteration\n",
      "The 22th 10000 iteration\n",
      "The 23th 10000 iteration\n",
      "The 24th 10000 iteration\n",
      "The 25th 10000 iteration\n",
      "The 26th 10000 iteration\n",
      "The 27th 10000 iteration\n",
      "The 28th 10000 iteration\n",
      "The 29th 10000 iteration\n",
      "The 30th 10000 iteration\n",
      "The 31th 10000 iteration\n",
      "The 32th 10000 iteration\n",
      "The 33th 10000 iteration\n",
      "The 34th 10000 iteration\n",
      "The 35th 10000 iteration\n",
      "The 36th 10000 iteration\n",
      "The 37th 10000 iteration\n",
      "The 38th 10000 iteration\n",
      "The 39th 10000 iteration\n",
      "The 40th 10000 iteration\n",
      "The 41th 10000 iteration\n",
      "The 42th 10000 iteration\n",
      "The 43th 10000 iteration\n",
      "The 44th 10000 iteration\n"
     ]
    }
   ],
   "source": [
    "# conbine and pre_process tweets\n",
    "tweets = []\n",
    "for filename in ['seed_user_tweets.txt','followers_tweets.txt','friends_tweets.txt']:\n",
    "    with open(os.path.join(data_dir, filename).encode('utf-8')) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            postprocess_tweet = []\n",
    "            try:\n",
    "                tweet = line.strip().split('::::')[1]\n",
    "            except:\n",
    "                continue\n",
    "            tweet = pre_process(tweet)\n",
    "            if i%10000==0:\n",
    "                print 'The %sth 10000 iteration'%(i/10000)\n",
    "            for word in tweet:\n",
    "                if word not in stops:\n",
    "                    postprocess_tweet.append(word)\n",
    "            tweets.append(' '.join(postprocess_tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def is_not_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return False\n",
    "    except ValueError:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 0th 10000 iteration\n",
      "The 1th 10000 iteration\n",
      "The 2th 10000 iteration\n",
      "The 3th 10000 iteration\n",
      "The 4th 10000 iteration\n",
      "The 5th 10000 iteration\n",
      "The 6th 10000 iteration\n",
      "The 7th 10000 iteration\n",
      "The 8th 10000 iteration\n",
      "The 9th 10000 iteration\n",
      "The 10th 10000 iteration\n",
      "The 11th 10000 iteration\n",
      "The 12th 10000 iteration\n",
      "The 13th 10000 iteration\n",
      "The 14th 10000 iteration\n",
      "The 15th 10000 iteration\n",
      "The 16th 10000 iteration\n",
      "The 17th 10000 iteration\n",
      "The 18th 10000 iteration\n",
      "The 19th 10000 iteration\n",
      "The 20th 10000 iteration\n",
      "The 21th 10000 iteration\n",
      "The 22th 10000 iteration\n",
      "The 23th 10000 iteration\n",
      "The 24th 10000 iteration\n",
      "The 25th 10000 iteration\n",
      "The 26th 10000 iteration\n",
      "The 27th 10000 iteration\n",
      "The 28th 10000 iteration\n",
      "The 29th 10000 iteration\n",
      "The 30th 10000 iteration\n",
      "The 31th 10000 iteration\n",
      "The 32th 10000 iteration\n",
      "The 33th 10000 iteration\n",
      "The 34th 10000 iteration\n",
      "The 35th 10000 iteration\n",
      "The 36th 10000 iteration\n",
      "The 37th 10000 iteration\n",
      "The 38th 10000 iteration\n",
      "The 0th 10000 iteration\n",
      "The 1th 10000 iteration\n",
      "The 2th 10000 iteration\n",
      "The 3th 10000 iteration\n",
      "The 4th 10000 iteration\n",
      "The 5th 10000 iteration\n",
      "The 6th 10000 iteration\n",
      "The 7th 10000 iteration\n",
      "The 8th 10000 iteration\n",
      "The 9th 10000 iteration\n",
      "The 10th 10000 iteration\n",
      "The 11th 10000 iteration\n",
      "The 12th 10000 iteration\n",
      "The 13th 10000 iteration\n",
      "The 14th 10000 iteration\n",
      "The 15th 10000 iteration\n",
      "The 16th 10000 iteration\n",
      "The 17th 10000 iteration\n",
      "The 18th 10000 iteration\n",
      "The 19th 10000 iteration\n",
      "The 20th 10000 iteration\n",
      "The 21th 10000 iteration\n",
      "The 22th 10000 iteration\n",
      "The 23th 10000 iteration\n",
      "The 24th 10000 iteration\n",
      "The 25th 10000 iteration\n",
      "The 26th 10000 iteration\n",
      "The 27th 10000 iteration\n",
      "The 28th 10000 iteration\n",
      "The 29th 10000 iteration\n",
      "The 0th 10000 iteration\n",
      "The 1th 10000 iteration\n",
      "The 2th 10000 iteration\n",
      "The 3th 10000 iteration\n",
      "The 4th 10000 iteration\n",
      "The 5th 10000 iteration\n",
      "The 6th 10000 iteration\n",
      "The 7th 10000 iteration\n",
      "The 8th 10000 iteration\n",
      "The 9th 10000 iteration\n",
      "The 10th 10000 iteration\n",
      "The 11th 10000 iteration\n",
      "The 12th 10000 iteration\n",
      "The 13th 10000 iteration\n",
      "The 14th 10000 iteration\n",
      "The 15th 10000 iteration\n",
      "The 16th 10000 iteration\n",
      "The 17th 10000 iteration\n",
      "The 18th 10000 iteration\n",
      "The 19th 10000 iteration\n",
      "The 20th 10000 iteration\n",
      "The 21th 10000 iteration\n",
      "The 22th 10000 iteration\n",
      "The 23th 10000 iteration\n",
      "The 24th 10000 iteration\n",
      "The 25th 10000 iteration\n",
      "The 26th 10000 iteration\n",
      "The 27th 10000 iteration\n",
      "The 28th 10000 iteration\n",
      "The 29th 10000 iteration\n",
      "The 30th 10000 iteration\n",
      "The 31th 10000 iteration\n",
      "The 32th 10000 iteration\n",
      "The 33th 10000 iteration\n",
      "The 34th 10000 iteration\n",
      "The 35th 10000 iteration\n",
      "The 36th 10000 iteration\n",
      "The 37th 10000 iteration\n",
      "The 38th 10000 iteration\n",
      "The 39th 10000 iteration\n",
      "The 40th 10000 iteration\n",
      "The 41th 10000 iteration\n",
      "The 42th 10000 iteration\n",
      "The 43th 10000 iteration\n",
      "The 44th 10000 iteration\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# read all tweets of followers and friends\n",
    "uid_list = []\n",
    "for filename in ['seed_user_tweets.txt','followers_tweets.txt','friends_tweets.txt']:\n",
    "    with open(os.path.join(data_dir, filename).encode('utf-8')) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i%10000==0:\n",
    "                print 'The %sth 10000 iteration'%(i/10000)\n",
    "            try:\n",
    "                tweet = line.strip().split('::::')[1]\n",
    "            except:\n",
    "                continue\n",
    "            uid = line.strip().split('::::')[0]\n",
    "            uid_list.append(uid)\n",
    "\n",
    "## save them to a file\n",
    "fout = open(os.path.join(data_dir, 'tweets_all_processed.txt').encode('utf-8'), 'w')\n",
    "tweets_new = []\n",
    "for t,tweet in enumerate(tweets):\n",
    "    words = tweet.split(' ')\n",
    "    new = []\n",
    "    for word in words:\n",
    "        if isEnglish(word)&is_not_number(word):\n",
    "            new.append(word)\n",
    "    new_tweet = ' '.join(new)\n",
    "    id_tweet = uid_list[t]+'::::'+new_tweet\n",
    "    fout.write('%s\\n' %id_tweet)\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268928\n"
     ]
    }
   ],
   "source": [
    "# preparing docs for LDA\n",
    "x = []\n",
    "with open(os.path.join(data_dir, 'tweets_all_processed.txt').encode('utf-8')) as f:\n",
    "    for i, line in enumerate(f):\n",
    "        tweets = line.strip().split('::::')[1]\n",
    "        x.append(tweets)\n",
    "# keep only informative tweets\n",
    "x = [i for i in x if (len(i.split(' '))>5)]\n",
    "print len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features...\n",
      "(268928, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting features...\")\n",
    "x_vect = CountVectorizer(max_df=0.95, min_df=1,max_features = 10000,stop_words='english')\n",
    "x_feats = x_vect.fit_transform(x)\n",
    "x_feats_names = x_vect.get_feature_names()\n",
    "print(x_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "amazing family working friends friend games home birthday single talking business person weve fact members hair matter bc record honor\n",
      "Topic 1:\n",
      "video trump season team story media pay american office michael artist space breaking action john app director trumps justice fbi\n",
      "Topic 2:\n",
      "women hes ill president men id fight human children gt car baby hot starting sex law bitch hell united china\n",
      "Topic 3:\n",
      "ive community party job public chance police lost film facebook email london data campaign kid student key loved stories eyes\n",
      "Topic 4:\n",
      "favorite city song kids power gun photo code movie dm phone youve league sale uk syria shop shooting meeting texas\n",
      "Topic 5:\n",
      "game ur fans national shot weekend todays star class james west players information dog stream instagram running science cardi skin\n",
      "Topic 6:\n",
      "na gon coming students wan giving college king body group living ta youll nba points thoughts reading security research room\n",
      "Topic 7:\n",
      "amp world book history future series war player country art team career questions info child tv message woman brand age\n",
      "Topic 8:\n",
      "album episode 1st official incredible states government goal program parents energy education greatest university sweet success chicago performance russia knew\n",
      "Topic 9:\n",
      "music house tickets tour food heres south america super lil words moment playing youtube violence rights paul yo bts blue\n"
     ]
    }
   ],
   "source": [
    "n_components  = 10\n",
    "\n",
    "# Run LDA\n",
    "lda = LatentDirichletAllocation(n_components=n_components, \n",
    "                                max_iter=5, \n",
    "                                learning_method='online', \n",
    "                                learning_offset=50.,random_state=0)\n",
    "# train LDA\n",
    "lda = lda.fit(x_feats)\n",
    "\n",
    "# display the words distribution among topics \n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print \"Topic %d:\" % (topic_idx+1)\n",
    "        print \" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]])\n",
    "\n",
    "no_top_words = 20\n",
    "display_topics(lda, x_feats_names, no_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second**, we predict for each user (seed, friends or followers), what's the topic distribution of him or her and save it as a vector(topic0:prob0; topic1:prob1; ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function for lda prediction, the output is the topic probability distribution of each user\n",
    "new_vect = CountVectorizer(vocabulary=x_feats_names)\n",
    "def predict_lda(group_of_tweets):\n",
    "    new_feats = new_vect.fit_transform(group_of_tweets)\n",
    "    predict = lda.transform(new_feats)\n",
    "    pred = predict.mean(axis=0)\n",
    "    return list(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate topic distribution of **seed** users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start loading and process data...\n",
      "The 0th 100000 iteration\n",
      "The 1th 100000 iteration\n",
      "The 2th 100000 iteration\n",
      "The 3th 100000 iteration\n",
      "The 4th 100000 iteration\n",
      "The 5th 100000 iteration\n",
      "The 6th 100000 iteration\n",
      "The 7th 100000 iteration\n",
      "The 8th 100000 iteration\n",
      "The 9th 100000 iteration\n",
      "The 10th 100000 iteration\n",
      "The 11th 100000 iteration\n",
      "Process completed.\n"
     ]
    }
   ],
   "source": [
    "data_dir = './final data'  ##Setting your own file path here.\n",
    "tweets = 'tweets_all_processed.txt'\n",
    "\n",
    "print('start loading and process data...')\n",
    "\n",
    "tweets_group = []\n",
    "topic_distribution_list = []\n",
    "uid0 = '776681696624345088'\n",
    "left_out = 0\n",
    "with open(os.path.join(data_dir, tweets).encode('utf-8'),'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        tweet_obj = line.strip().split('::::')\n",
    "        uid = tweet_obj[0]\n",
    "        tweet = tweet_obj[1]\n",
    "        \n",
    "#         # remove empty tweets\n",
    "#         if tweet =='':\n",
    "#             left_out+=1\n",
    "#             continue\n",
    "#         else:\n",
    "#             pass\n",
    "        \n",
    "        if i%100000==0:\n",
    "            print 'The %sth 100000 iteration'%(i/100000)\n",
    "            \n",
    "        if uid == uid0:\n",
    "            tweets_group.append(tweet)\n",
    "        else:\n",
    "            \n",
    "            # keep users with more than 50 valid tweets\n",
    "            if len(tweets_group)>2:\n",
    "                pass\n",
    "            else:\n",
    "                left_out+=1\n",
    "                uid0=uid\n",
    "                tweets_group = []\n",
    "                continue\n",
    "                \n",
    "            topic_distribution = predict_lda(tweets_group)\n",
    "            id_topic_distribution = uid0+'::::'+json.dumps(topic_distribution)\n",
    "            topic_distribution_list.append(id_topic_distribution)\n",
    "            \n",
    "            uid0=uid\n",
    "            tweets_group = []\n",
    "        \n",
    "\n",
    "\n",
    "fout = open(os.path.join(data_dir, 'seed_user_distribution.txt').encode('utf-8'), 'w')# from 5304: followers; from 8652: friends\n",
    "for i in topic_distribution_list[:5303]:\n",
    "    fout.write('%s\\n' %i)\n",
    "fout.close()\n",
    "\n",
    "fout = open(os.path.join(data_dir, 'follower_distribution.txt').encode('utf-8'), 'w')# from 5304: followers; from 8652: friends\n",
    "for i in topic_distribution_list[5304:8651]:\n",
    "    fout.write('%s\\n' %i)\n",
    "fout.close()\n",
    "\n",
    "fout = open(os.path.join(data_dir, 'friend_distribution.txt').encode('utf-8'), 'w')# from 5304: followers; from 8652: friends\n",
    "for i in topic_distribution_list[8652:]:\n",
    "    fout.write('%s\\n' %i)\n",
    "fout.close()\n",
    "\n",
    "print('Process completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13699"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(topic_distribution_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Third**, calculate similarity between each user and their friends and followers, output the top 10 most similar users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(vect1,vect2):\n",
    "    result = 1 - spatial.distance.cosine(vect1,vect2)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take 7 users as example: 836758265824423936, 2677264639,  809056262432391168, 3161727481,  3260137987, 53185280, 833064800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# look up follower and friend id from tables\n",
    "input_uid = '2677264639'\n",
    "import pandas as pd\n",
    "followers = pd.read_pickle('./final data/followers_filtered')\n",
    "friends = pd.read_pickle('./final data/friends_filtered')\n",
    "follower_ids = followers.loc[followers['seed_user']==int(input_uid)]['idstr']\n",
    "follower_ids = follower_ids.tolist()\n",
    "follower_ids = [str(i).replace('L','') for i in follower_ids]\n",
    "\n",
    "friend_ids = friends.loc[friends['seed_user']==int(input_uid)]['idstr']\n",
    "friend_ids = friend_ids.tolist()\n",
    "friend_ids = [str(i).replace('L','') for i in friend_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the accuracy of similarity based classification (recommend when similarity higher than 0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3370\n",
      "[('72665173', 0.9855692580977835), ('2938100399', 0.9784753772737028), ('625283075', 0.977270998435106), ('3631362339', 0.9753947524029087), ('573181047', 0.9715385166391677), ('3290208761', 0.9658643345438105), ('174930501', 0.9657654560480828), ('2369696214', 0.9626900182095447), ('19091405', 0.9610259221838785), ('4901673680', 0.9552355764649323), ('814418096', 0.9422272307231158), ('3378043343', 0.9376650785263239), ('499202239', 0.9303848876131111), ('33612317', 0.9222368697976674), ('1014178490', 0.9130825632674969), ('32712325', 0.8323133274344572)]\n",
      "[('1400748608', 0.9831191151903111), ('783108378742562816', 0.9809193069965703), ('1056487393', 0.9790007662277302), ('181572333', 0.9683051825448191), ('3950477674', 0.9661999741485021), ('95023423', 0.9609497105212569), ('112540334', 0.9576612658557357), ('78525538', 0.9553211649683933), ('866953267', 0.9344377926063014), ('152457403', 0.9018429921867455)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1400748608',\n",
       " '783108378742562816',\n",
       " '1056487393',\n",
       " '181572333',\n",
       " '3950477674',\n",
       " '95023423',\n",
       " '112540334',\n",
       " '78525538',\n",
       " '866953267']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = './final data'\n",
    "with open(os.path.join(data_dir, 'seed_user_distribution.txt').encode('utf-8')) as f:\n",
    "    for i, line in enumerate(f):\n",
    "        obj = line.strip().split('::::')\n",
    "        uid = obj[0]\n",
    "        seed_vect = json.loads(obj[1])\n",
    "        if str(uid) == str(input_uid):\n",
    "            print i\n",
    "            break\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "# these are users that follow the seed user but are not followed back, which we believe are the type we shouldn't recommend\n",
    "follower_similarity_dict = {}\n",
    "with open(os.path.join(data_dir, 'follower_distribution.txt').encode('utf-8')) as f:\n",
    "    for i, line in enumerate(f):\n",
    "        obj = line.strip().split('::::')\n",
    "        uid = obj[0]\n",
    "        if uid in follower_ids:\n",
    "            pass\n",
    "        else:\n",
    "            continue\n",
    "        follower_vect = json.loads(obj[1])\n",
    "        sim = calculate_similarity(seed_vect,follower_vect)\n",
    "        follower_similarity_dict[uid] = sim\n",
    "\n",
    "# these are users that are followed the seed user but not follow back, which we believe are the type we should recommend     \n",
    "friend_similarity_dict = {}\n",
    "with open(os.path.join(data_dir, 'friend_distribution.txt').encode('utf-8')) as f:\n",
    "    for i, line in enumerate(f):\n",
    "        obj = line.strip().split('::::')\n",
    "        uid = obj[0]\n",
    "        if uid in friend_ids:\n",
    "            pass\n",
    "        else:\n",
    "            continue\n",
    "        friend_vect = json.loads(obj[1])\n",
    "        sim = calculate_similarity(seed_vect,friend_vect)\n",
    "        friend_similarity_dict[uid] = sim\n",
    "\n",
    "sorted_followers = sorted(follower_similarity_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "print sorted_followers\n",
    "\n",
    "sorted_friends = sorted(friend_similarity_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "print sorted_friends\n",
    "\n",
    "try:\n",
    "    recommend = sorted_friends[:9]\n",
    "except:\n",
    "    recommend = sorted_friends\n",
    "\n",
    "recommend_id_list = []\n",
    "for r in recommend:\n",
    "    recommend_id_list.append(r[0])\n",
    "recommend_id_list # recommend users from friend list and follower list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2677264639', 1.0), ('939380029', 0.9966474420774795), ('513917014', 0.9955926016126856), ('1932382154', 0.9955440854063033), ('1692679680', 0.9948573151752046), ('527754604', 0.9947138747558425), ('946564248', 0.994358004234309), ('614053008', 0.9943477482259695), ('621358322', 0.994105343837573)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2677264639',\n",
       " '939380029',\n",
       " '513917014',\n",
       " '1932382154',\n",
       " '1692679680',\n",
       " '527754604',\n",
       " '946564248',\n",
       " '614053008',\n",
       " '621358322']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_similarity_dict = {}\n",
    "with open(os.path.join(data_dir, 'all_topic_distribution.txt').encode('utf-8')) as f:\n",
    "    for i, line in enumerate(f):\n",
    "        obj = line.strip().split('::::')\n",
    "        uid = obj[0]\n",
    "        all_vect = json.loads(obj[1])\n",
    "        sim = calculate_similarity(seed_vect,all_vect)\n",
    "        all_similarity_dict[uid] = sim\n",
    "\n",
    "sorted_all = sorted(all_similarity_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "print sorted_all[:9]\n",
    "\n",
    "try:\n",
    "    recommend = sorted_all[:9]\n",
    "except:\n",
    "    recommend = sorted_all\n",
    "\n",
    "recommend_id_list = []\n",
    "for r in recommend:\n",
    "    recommend_id_list.append(r[0])\n",
    "recommend_id_list # recommend users from full candidate list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profile-based recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict whether a seed user will follow another user**\n",
    "\n",
    "seed_user = [392488192,45133149,376987007,22513648,3718133833,30140207,361067987,35832236,896663059]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(522, 11) (1142, 11) (575, 9)\n"
     ]
    }
   ],
   "source": [
    "names = ['seed_user_id','id','name','verified','statuses_cnt','followers_cnt','friends_cnt','listed_cnt','favourites_cnt','description']\n",
    "followers = pd.read_csv('./final data/big train/bigtrain_followers.txt',sep='::::',header=None,names=names,engine='python')\n",
    "\n",
    "names = ['seed_user_id','id','name','verified','statuses_cnt','followers_cnt','friends_cnt','listed_cnt','favourites_cnt','description']\n",
    "friends = pd.read_csv('./final data/big train/bigtrain_friends.txt',sep='::::',header=None,names=names,engine='python')\n",
    "\n",
    "names = ['id','name','verified','statuses_cnt','followers_cnt','friends_cnt','listed_cnt','favourites_cnt','description']\n",
    "seed_users = pd.read_csv('./final data/seed_user_profile.txt',sep='::::',header=None,names=names,engine='python')\n",
    "\n",
    "followers['label'] = 0\n",
    "friends['label'] = 1 \n",
    "print followers.shape,friends.shape,seed_users.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44, 11) (9, 11)\n"
     ]
    }
   ],
   "source": [
    "input_uid = 896663059\n",
    "followers_input = followers.loc[followers['seed_user_id']==input_uid]\n",
    "friends_input = friends.loc[friends['seed_user_id']==input_uid]\n",
    "print followers_input.shape, friends_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# followers_input = pd.merge(followers_input,followers_group)\n",
    "# friends_input = pd.merge(friends_input,friends_group)\n",
    "# print followers_input.shape, friends_input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process df: 1. concatenate 2 df; 2. feature engineering; 3. transform into matrix for sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53, 11) Index([u'seed_user_id', u'id', u'name', u'verified', u'statuses_cnt',\n",
      "       u'followers_cnt', u'friends_cnt', u'listed_cnt', u'favourites_cnt',\n",
      "       u'description', u'label'],\n",
      "      dtype='object')\n",
      "(53, 7) Index([u'verified', u'statuses_cnt', u'followers_cnt', u'friends_cnt',\n",
      "       u'listed_cnt', u'favourites_cnt', u'label'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "## labeling\n",
    "input_df = pd.concat([followers_input, friends_input], axis=0)\n",
    "print input_df.shape,input_df.columns\n",
    "\n",
    "input_df_new = input_df.drop(['seed_user_id','id','name','description'], axis=1)\n",
    "print input_df_new.shape,input_df_new.columns\n",
    "\n",
    "input_array = np.array(input_df_new.iloc[:,:6])\n",
    "y = np.array(input_df_new['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training and predict...\n",
      "Average Precision of svm is 0.710707.\n",
      "Average Recall of svm is 0.752778.\n",
      "Average Accuracy of svm is 0.849091.\n"
     ]
    }
   ],
   "source": [
    "print(\"Start training and predict...\")\n",
    "kf = KFold(n_splits=5,shuffle=True) ## function to spilt train and test data set\n",
    "avg_p = 0\n",
    "avg_r = 0\n",
    "avg_a = 0\n",
    "for train, test in kf.split(input_array):\n",
    "    model = LogisticRegression().fit(input_array[train], y[train])\n",
    "    predicts = model.predict(input_array[test])\n",
    "#     print(classification_report(y[test],predicts))\n",
    "    avg_p += precision_score(y[test],predicts, average='macro',pos_label=1)\n",
    "    avg_r += recall_score(y[test],predicts, average='macro',pos_label=1)\n",
    "    avg_a += accuracy_score(y[test],predicts)\n",
    "\n",
    "print('Average Precision of svm is %f.' %(avg_p/5))\n",
    "print('Average Recall of svm is %f.' %(avg_r/5))\n",
    "print('Average Accuracy of svm is %f.' %(avg_a/5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict on all users by this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2239, 6)\n"
     ]
    }
   ],
   "source": [
    "input_test = pd.concat([followers.iloc[:,1:9], friends.iloc[:,1:9], seed_users], axis=0)\n",
    "input_test_new = input_test.drop(['id','name','description'], axis=1)\n",
    "print input_test_new.shape\n",
    "\n",
    "x_feats_all = np.array(input_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1] [[0.00000000e+00 1.00000000e+00]\n",
      " [0.00000000e+00 1.00000000e+00]\n",
      " [4.66200363e-04 9.99533800e-01]\n",
      " ...\n",
      " [0.00000000e+00 1.00000000e+00]\n",
      " [0.00000000e+00 1.00000000e+00]\n",
      " [0.00000000e+00 1.00000000e+00]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>favourites_cnt</th>\n",
       "      <th>followers_cnt</th>\n",
       "      <th>friends_cnt</th>\n",
       "      <th>id</th>\n",
       "      <th>listed_cnt</th>\n",
       "      <th>name</th>\n",
       "      <th>statuses_cnt</th>\n",
       "      <th>verified</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1602</td>\n",
       "      <td>625</td>\n",
       "      <td>373</td>\n",
       "      <td>856264641722195970</td>\n",
       "      <td>3</td>\n",
       "      <td>punflower</td>\n",
       "      <td>797</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5254</td>\n",
       "      <td>70576</td>\n",
       "      <td>11877</td>\n",
       "      <td>722210815110041600</td>\n",
       "      <td>367</td>\n",
       "      <td>HandmaidsOnHulu</td>\n",
       "      <td>2992</td>\n",
       "      <td>True</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>149</td>\n",
       "      <td>6180</td>\n",
       "      <td>3786</td>\n",
       "      <td>922634896207687685</td>\n",
       "      <td>2</td>\n",
       "      <td>vaakoh</td>\n",
       "      <td>154</td>\n",
       "      <td>False</td>\n",
       "      <td>0.999534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>392</td>\n",
       "      <td>465</td>\n",
       "      <td>374</td>\n",
       "      <td>3837479480</td>\n",
       "      <td>0</td>\n",
       "      <td>lift_momentum</td>\n",
       "      <td>243</td>\n",
       "      <td>False</td>\n",
       "      <td>0.999988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>12654</td>\n",
       "      <td>2362</td>\n",
       "      <td>1707</td>\n",
       "      <td>361695170</td>\n",
       "      <td>135</td>\n",
       "      <td>JenniNexus</td>\n",
       "      <td>7788</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>32170</td>\n",
       "      <td>3536</td>\n",
       "      <td>2528</td>\n",
       "      <td>725350782497906688</td>\n",
       "      <td>181</td>\n",
       "      <td>RoanokeMaven</td>\n",
       "      <td>11622</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>33396</td>\n",
       "      <td>102131</td>\n",
       "      <td>45311</td>\n",
       "      <td>1584118603</td>\n",
       "      <td>253</td>\n",
       "      <td>FromAshestoNew</td>\n",
       "      <td>5981</td>\n",
       "      <td>True</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>12851</td>\n",
       "      <td>1374</td>\n",
       "      <td>754</td>\n",
       "      <td>169604573</td>\n",
       "      <td>61</td>\n",
       "      <td>Lee_G_Malone</td>\n",
       "      <td>9931</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>99</td>\n",
       "      <td>53</td>\n",
       "      <td>703927794150055936</td>\n",
       "      <td>6</td>\n",
       "      <td>UncappingBadger</td>\n",
       "      <td>666</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>17408</td>\n",
       "      <td>13840</td>\n",
       "      <td>1689</td>\n",
       "      <td>90995027</td>\n",
       "      <td>86</td>\n",
       "      <td>samzorz</td>\n",
       "      <td>16467</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1277</td>\n",
       "      <td>6205</td>\n",
       "      <td>2200</td>\n",
       "      <td>28719244</td>\n",
       "      <td>106</td>\n",
       "      <td>bunnyXablaze</td>\n",
       "      <td>17950</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>6796</td>\n",
       "      <td>4134</td>\n",
       "      <td>931</td>\n",
       "      <td>2265419863</td>\n",
       "      <td>18</td>\n",
       "      <td>Tentanman_</td>\n",
       "      <td>4762</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3144</td>\n",
       "      <td>18371</td>\n",
       "      <td>15220</td>\n",
       "      <td>173631389</td>\n",
       "      <td>427</td>\n",
       "      <td>lowcarbyum</td>\n",
       "      <td>27213</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9299</td>\n",
       "      <td>899</td>\n",
       "      <td>508</td>\n",
       "      <td>189762228</td>\n",
       "      <td>26</td>\n",
       "      <td>AzeraW84</td>\n",
       "      <td>10883</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1023</td>\n",
       "      <td>397</td>\n",
       "      <td>356</td>\n",
       "      <td>3515236817</td>\n",
       "      <td>27</td>\n",
       "      <td>KaraCosplayer</td>\n",
       "      <td>1815</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>15873</td>\n",
       "      <td>2090</td>\n",
       "      <td>511</td>\n",
       "      <td>753309767343505409</td>\n",
       "      <td>26</td>\n",
       "      <td>noelledelray</td>\n",
       "      <td>2853</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>6652</td>\n",
       "      <td>529</td>\n",
       "      <td>273</td>\n",
       "      <td>418142882</td>\n",
       "      <td>14</td>\n",
       "      <td>Botulism_Sauce</td>\n",
       "      <td>4654</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>32714</td>\n",
       "      <td>896</td>\n",
       "      <td>614</td>\n",
       "      <td>2804724307</td>\n",
       "      <td>8</td>\n",
       "      <td>TheSabaiGuy</td>\n",
       "      <td>5409</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10308</td>\n",
       "      <td>1976</td>\n",
       "      <td>498</td>\n",
       "      <td>235665226</td>\n",
       "      <td>88</td>\n",
       "      <td>ChapperzTV</td>\n",
       "      <td>20033</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>851</td>\n",
       "      <td>448</td>\n",
       "      <td>111</td>\n",
       "      <td>3725804656</td>\n",
       "      <td>4</td>\n",
       "      <td>dlmgaming</td>\n",
       "      <td>1242</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>43198</td>\n",
       "      <td>898</td>\n",
       "      <td>703</td>\n",
       "      <td>707336817758965760</td>\n",
       "      <td>18</td>\n",
       "      <td>PostCubicleKyle</td>\n",
       "      <td>13684</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>48559</td>\n",
       "      <td>11728</td>\n",
       "      <td>7601</td>\n",
       "      <td>554482039</td>\n",
       "      <td>37</td>\n",
       "      <td>ImShavy</td>\n",
       "      <td>44325</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5700</td>\n",
       "      <td>5153</td>\n",
       "      <td>309</td>\n",
       "      <td>126391589</td>\n",
       "      <td>59</td>\n",
       "      <td>VeeFergie</td>\n",
       "      <td>39896</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>77908</td>\n",
       "      <td>9671</td>\n",
       "      <td>983</td>\n",
       "      <td>2790513522</td>\n",
       "      <td>109</td>\n",
       "      <td>TheHunterWildTV</td>\n",
       "      <td>37653</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>110</td>\n",
       "      <td>3884</td>\n",
       "      <td>3038</td>\n",
       "      <td>793889939154796544</td>\n",
       "      <td>9</td>\n",
       "      <td>blackdoggaming</td>\n",
       "      <td>252</td>\n",
       "      <td>False</td>\n",
       "      <td>0.999994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3807</td>\n",
       "      <td>4332</td>\n",
       "      <td>2673</td>\n",
       "      <td>51767621</td>\n",
       "      <td>27</td>\n",
       "      <td>itsamyactually</td>\n",
       "      <td>33742</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1079</td>\n",
       "      <td>334</td>\n",
       "      <td>120</td>\n",
       "      <td>2662005618</td>\n",
       "      <td>13</td>\n",
       "      <td>GimmeGoblin</td>\n",
       "      <td>8727</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>297</td>\n",
       "      <td>79852</td>\n",
       "      <td>54989</td>\n",
       "      <td>4378695496</td>\n",
       "      <td>19</td>\n",
       "      <td>HullDanh10118</td>\n",
       "      <td>16372</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>7247</td>\n",
       "      <td>189</td>\n",
       "      <td>89</td>\n",
       "      <td>1394048688</td>\n",
       "      <td>4</td>\n",
       "      <td>Cupkacenom</td>\n",
       "      <td>6551</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>11355</td>\n",
       "      <td>1277</td>\n",
       "      <td>1018</td>\n",
       "      <td>3846671002</td>\n",
       "      <td>34</td>\n",
       "      <td>BrickinNick</td>\n",
       "      <td>5224</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>Scientist, sci-comm, slug-slayer, director, OU...</td>\n",
       "      <td>395</td>\n",
       "      <td>276</td>\n",
       "      <td>184</td>\n",
       "      <td>351624206</td>\n",
       "      <td>30</td>\n",
       "      <td>Dr_PamC</td>\n",
       "      <td>4682</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>Enjoy Street Magic.  Streetwear for Sk...</td>\n",
       "      <td>3664</td>\n",
       "      <td>225</td>\n",
       "      <td>117</td>\n",
       "      <td>825648227877847040</td>\n",
       "      <td>0</td>\n",
       "      <td>SXRCXRY</td>\n",
       "      <td>1031</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>forty five quid</td>\n",
       "      <td>6764</td>\n",
       "      <td>270</td>\n",
       "      <td>188</td>\n",
       "      <td>822963670623879170</td>\n",
       "      <td>1</td>\n",
       "      <td>WraightDanielle</td>\n",
       "      <td>2501</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>A sports podcast focused primarily on MSU/UM/N...</td>\n",
       "      <td>10615</td>\n",
       "      <td>631</td>\n",
       "      <td>394</td>\n",
       "      <td>836758265824423936</td>\n",
       "      <td>15</td>\n",
       "      <td>TheUNOVisit</td>\n",
       "      <td>14412</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td> UWB     Did you nourish your body today?</td>\n",
       "      <td>19805</td>\n",
       "      <td>440</td>\n",
       "      <td>284</td>\n",
       "      <td>391743807</td>\n",
       "      <td>2</td>\n",
       "      <td>y3llowChaoz</td>\n",
       "      <td>5027</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>astrophile // army // aus // ask me anything</td>\n",
       "      <td>2098</td>\n",
       "      <td>158</td>\n",
       "      <td>48</td>\n",
       "      <td>911043434797973504</td>\n",
       "      <td>0</td>\n",
       "      <td>thirstaeforjeon</td>\n",
       "      <td>1112</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>Business Education Teacher. Lover of corgis, t...</td>\n",
       "      <td>5130</td>\n",
       "      <td>174</td>\n",
       "      <td>129</td>\n",
       "      <td>27395294</td>\n",
       "      <td>2</td>\n",
       "      <td>jamiejo2005</td>\n",
       "      <td>1751</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>They say love is pain, well darling, let's hur...</td>\n",
       "      <td>9898</td>\n",
       "      <td>856</td>\n",
       "      <td>470</td>\n",
       "      <td>53185280</td>\n",
       "      <td>44</td>\n",
       "      <td>StephanieHakim</td>\n",
       "      <td>49752</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>Music, Sneakers, Motorcycles.. not necessarily...</td>\n",
       "      <td>258</td>\n",
       "      <td>634</td>\n",
       "      <td>337</td>\n",
       "      <td>19627587</td>\n",
       "      <td>10</td>\n",
       "      <td>1andee</td>\n",
       "      <td>1868</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>Springfield College '18</td>\n",
       "      <td>16564</td>\n",
       "      <td>463</td>\n",
       "      <td>332</td>\n",
       "      <td>368675894</td>\n",
       "      <td>1</td>\n",
       "      <td>MattySimmons_</td>\n",
       "      <td>8748</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>Health &amp; fitness | CSEP-CPT</td>\n",
       "      <td>2746</td>\n",
       "      <td>308</td>\n",
       "      <td>302</td>\n",
       "      <td>1486278750</td>\n",
       "      <td>0</td>\n",
       "      <td>rebeccaeagless</td>\n",
       "      <td>3830</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>NaN</td>\n",
       "      <td>13917</td>\n",
       "      <td>570</td>\n",
       "      <td>329</td>\n",
       "      <td>351980190</td>\n",
       "      <td>3</td>\n",
       "      <td>Stephanieee_12</td>\n",
       "      <td>9694</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>From that time on, the world was hers for the ...</td>\n",
       "      <td>9950</td>\n",
       "      <td>400</td>\n",
       "      <td>289</td>\n",
       "      <td>70368517</td>\n",
       "      <td>6</td>\n",
       "      <td>bluesKLUH</td>\n",
       "      <td>11108</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>Genderfluid, Artist, Furry, Gamer, Let's Playe...</td>\n",
       "      <td>23561</td>\n",
       "      <td>612</td>\n",
       "      <td>461</td>\n",
       "      <td>3260137987</td>\n",
       "      <td>1</td>\n",
       "      <td>MoonWuff</td>\n",
       "      <td>10859</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>Aspiring entertainment critic. Art enthusiast....</td>\n",
       "      <td>1073</td>\n",
       "      <td>844</td>\n",
       "      <td>241</td>\n",
       "      <td>222378167</td>\n",
       "      <td>7</td>\n",
       "      <td>__Leago</td>\n",
       "      <td>34817</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>sc// elizabetharanda</td>\n",
       "      <td>15204</td>\n",
       "      <td>183</td>\n",
       "      <td>79</td>\n",
       "      <td>3020309328</td>\n",
       "      <td>1</td>\n",
       "      <td>elizaaveth</td>\n",
       "      <td>1858</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>ig: hvneyomi</td>\n",
       "      <td>33030</td>\n",
       "      <td>719</td>\n",
       "      <td>388</td>\n",
       "      <td>172799500</td>\n",
       "      <td>3</td>\n",
       "      <td>AOK1YOMI</td>\n",
       "      <td>30584</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>This account is intended for EVERYONE, of ever...</td>\n",
       "      <td>4470</td>\n",
       "      <td>1281</td>\n",
       "      <td>163</td>\n",
       "      <td>421576691</td>\n",
       "      <td>8</td>\n",
       "      <td>phbisexuals</td>\n",
       "      <td>8842</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>Im good, like yay</td>\n",
       "      <td>2522</td>\n",
       "      <td>194</td>\n",
       "      <td>190</td>\n",
       "      <td>710217144546541569</td>\n",
       "      <td>0</td>\n",
       "      <td>yaystout</td>\n",
       "      <td>785</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td> Dominican NYC Empire State.</td>\n",
       "      <td>2998</td>\n",
       "      <td>670</td>\n",
       "      <td>278</td>\n",
       "      <td>62693227</td>\n",
       "      <td>1</td>\n",
       "      <td>Mikeangel__</td>\n",
       "      <td>11395</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>Tinkerer/Maker/Technophile. Manage Software De...</td>\n",
       "      <td>1001</td>\n",
       "      <td>362</td>\n",
       "      <td>264</td>\n",
       "      <td>12321902</td>\n",
       "      <td>44</td>\n",
       "      <td>greghuber</td>\n",
       "      <td>6301</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>#Muslim| #Pakistani | #SnookerLover |</td>\n",
       "      <td>3890</td>\n",
       "      <td>4796</td>\n",
       "      <td>268</td>\n",
       "      <td>3161727481</td>\n",
       "      <td>8</td>\n",
       "      <td>rashi714</td>\n",
       "      <td>41319</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>#GrogeGang 11K+ Members! Team Spaces Player - ...</td>\n",
       "      <td>3035</td>\n",
       "      <td>732</td>\n",
       "      <td>365</td>\n",
       "      <td>809056262432391168</td>\n",
       "      <td>4</td>\n",
       "      <td>GrogeGamingCR</td>\n",
       "      <td>1763</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>Frump God</td>\n",
       "      <td>49572</td>\n",
       "      <td>455</td>\n",
       "      <td>450</td>\n",
       "      <td>241958036</td>\n",
       "      <td>8</td>\n",
       "      <td>real_tennille</td>\n",
       "      <td>17493</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3326</td>\n",
       "      <td>109</td>\n",
       "      <td>105</td>\n",
       "      <td>888545298003496960</td>\n",
       "      <td>1</td>\n",
       "      <td>FaithAlexusss</td>\n",
       "      <td>1870</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>'If you can't find the one being hustled in th...</td>\n",
       "      <td>16221</td>\n",
       "      <td>631</td>\n",
       "      <td>97</td>\n",
       "      <td>837421351853924352</td>\n",
       "      <td>16</td>\n",
       "      <td>sugarshookie</td>\n",
       "      <td>19947</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>Young Entrepreneur | CEO of @CLEgivesback | ma...</td>\n",
       "      <td>3885</td>\n",
       "      <td>466</td>\n",
       "      <td>409</td>\n",
       "      <td>2677264639</td>\n",
       "      <td>4</td>\n",
       "      <td>lexdaqueen</td>\n",
       "      <td>5220</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>20 || snapchat: dianacarranza16 </td>\n",
       "      <td>7648</td>\n",
       "      <td>473</td>\n",
       "      <td>275</td>\n",
       "      <td>160314040</td>\n",
       "      <td>7</td>\n",
       "      <td>Dianalmc11</td>\n",
       "      <td>67141</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>Exclusive ladies wear boutique offering fabulo...</td>\n",
       "      <td>11</td>\n",
       "      <td>323</td>\n",
       "      <td>94</td>\n",
       "      <td>138181434</td>\n",
       "      <td>5</td>\n",
       "      <td>ShopatParis</td>\n",
       "      <td>2424</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>@ufc fighter. #amandabynes #capsicum #collagem...</td>\n",
       "      <td>3469</td>\n",
       "      <td>34602</td>\n",
       "      <td>346</td>\n",
       "      <td>363860448</td>\n",
       "      <td>272</td>\n",
       "      <td>AngelaMagana1</td>\n",
       "      <td>7074</td>\n",
       "      <td>True</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2239 rows  10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           description  favourites_cnt  \\\n",
       "0                                                  NaN            1602   \n",
       "1                                                  NaN            5254   \n",
       "2                                                  NaN             149   \n",
       "3                                                  NaN             392   \n",
       "4                                                  NaN           12654   \n",
       "5                                                  NaN           32170   \n",
       "6                                                  NaN           33396   \n",
       "7                                                  NaN           12851   \n",
       "8                                                  NaN              51   \n",
       "9                                                  NaN           17408   \n",
       "10                                                 NaN            1277   \n",
       "11                                                 NaN            6796   \n",
       "12                                                 NaN            3144   \n",
       "13                                                 NaN            9299   \n",
       "14                                                 NaN            1023   \n",
       "15                                                 NaN           15873   \n",
       "16                                                 NaN            6652   \n",
       "17                                                 NaN           32714   \n",
       "18                                                 NaN           10308   \n",
       "19                                                 NaN             851   \n",
       "20                                                 NaN           43198   \n",
       "21                                                 NaN           48559   \n",
       "22                                                 NaN            5700   \n",
       "23                                                 NaN           77908   \n",
       "24                                                 NaN             110   \n",
       "25                                                 NaN            3807   \n",
       "26                                                 NaN            1079   \n",
       "27                                                 NaN             297   \n",
       "28                                                 NaN            7247   \n",
       "29                                                 NaN           11355   \n",
       "..                                                 ...             ...   \n",
       "545  Scientist, sci-comm, slug-slayer, director, OU...             395   \n",
       "546  Enjoy Street Magic.  Streetwear for Sk...            3664   \n",
       "547                                    forty five quid            6764   \n",
       "548  A sports podcast focused primarily on MSU/UM/N...           10615   \n",
       "549     UWB     Did you nourish your body today?           19805   \n",
       "550     astrophile // army // aus // ask me anything            2098   \n",
       "551  Business Education Teacher. Lover of corgis, t...            5130   \n",
       "552  They say love is pain, well darling, let's hur...            9898   \n",
       "553  Music, Sneakers, Motorcycles.. not necessarily...             258   \n",
       "554                            Springfield College '18           16564   \n",
       "555                        Health & fitness | CSEP-CPT            2746   \n",
       "556                                                NaN           13917   \n",
       "557  From that time on, the world was hers for the ...            9950   \n",
       "558  Genderfluid, Artist, Furry, Gamer, Let's Playe...           23561   \n",
       "559  Aspiring entertainment critic. Art enthusiast....            1073   \n",
       "560                               sc// elizabetharanda           15204   \n",
       "561                                       ig: hvneyomi           33030   \n",
       "562  This account is intended for EVERYONE, of ever...            4470   \n",
       "563                                 Im good, like yay            2522   \n",
       "564                  Dominican NYC Empire State.            2998   \n",
       "565  Tinkerer/Maker/Technophile. Manage Software De...            1001   \n",
       "566              #Muslim| #Pakistani | #SnookerLover |            3890   \n",
       "567  #GrogeGang 11K+ Members! Team Spaces Player - ...            3035   \n",
       "568                                          Frump God           49572   \n",
       "569                                                NaN            3326   \n",
       "570  'If you can't find the one being hustled in th...           16221   \n",
       "571  Young Entrepreneur | CEO of @CLEgivesback | ma...            3885   \n",
       "572               20 || snapchat: dianacarranza16             7648   \n",
       "573  Exclusive ladies wear boutique offering fabulo...              11   \n",
       "574  @ufc fighter. #amandabynes #capsicum #collagem...            3469   \n",
       "\n",
       "     followers_cnt  friends_cnt                  id  listed_cnt  \\\n",
       "0              625          373  856264641722195970           3   \n",
       "1            70576        11877  722210815110041600         367   \n",
       "2             6180         3786  922634896207687685           2   \n",
       "3              465          374          3837479480           0   \n",
       "4             2362         1707           361695170         135   \n",
       "5             3536         2528  725350782497906688         181   \n",
       "6           102131        45311          1584118603         253   \n",
       "7             1374          754           169604573          61   \n",
       "8               99           53  703927794150055936           6   \n",
       "9            13840         1689            90995027          86   \n",
       "10            6205         2200            28719244         106   \n",
       "11            4134          931          2265419863          18   \n",
       "12           18371        15220           173631389         427   \n",
       "13             899          508           189762228          26   \n",
       "14             397          356          3515236817          27   \n",
       "15            2090          511  753309767343505409          26   \n",
       "16             529          273           418142882          14   \n",
       "17             896          614          2804724307           8   \n",
       "18            1976          498           235665226          88   \n",
       "19             448          111          3725804656           4   \n",
       "20             898          703  707336817758965760          18   \n",
       "21           11728         7601           554482039          37   \n",
       "22            5153          309           126391589          59   \n",
       "23            9671          983          2790513522         109   \n",
       "24            3884         3038  793889939154796544           9   \n",
       "25            4332         2673            51767621          27   \n",
       "26             334          120          2662005618          13   \n",
       "27           79852        54989          4378695496          19   \n",
       "28             189           89          1394048688           4   \n",
       "29            1277         1018          3846671002          34   \n",
       "..             ...          ...                 ...         ...   \n",
       "545            276          184           351624206          30   \n",
       "546            225          117  825648227877847040           0   \n",
       "547            270          188  822963670623879170           1   \n",
       "548            631          394  836758265824423936          15   \n",
       "549            440          284           391743807           2   \n",
       "550            158           48  911043434797973504           0   \n",
       "551            174          129            27395294           2   \n",
       "552            856          470            53185280          44   \n",
       "553            634          337            19627587          10   \n",
       "554            463          332           368675894           1   \n",
       "555            308          302          1486278750           0   \n",
       "556            570          329           351980190           3   \n",
       "557            400          289            70368517           6   \n",
       "558            612          461          3260137987           1   \n",
       "559            844          241           222378167           7   \n",
       "560            183           79          3020309328           1   \n",
       "561            719          388           172799500           3   \n",
       "562           1281          163           421576691           8   \n",
       "563            194          190  710217144546541569           0   \n",
       "564            670          278            62693227           1   \n",
       "565            362          264            12321902          44   \n",
       "566           4796          268          3161727481           8   \n",
       "567            732          365  809056262432391168           4   \n",
       "568            455          450           241958036           8   \n",
       "569            109          105  888545298003496960           1   \n",
       "570            631           97  837421351853924352          16   \n",
       "571            466          409          2677264639           4   \n",
       "572            473          275           160314040           7   \n",
       "573            323           94           138181434           5   \n",
       "574          34602          346           363860448         272   \n",
       "\n",
       "                name  statuses_cnt  verified      prob  \n",
       "0          punflower           797     False  1.000000  \n",
       "1    HandmaidsOnHulu          2992      True  1.000000  \n",
       "2             vaakoh           154     False  0.999534  \n",
       "3      lift_momentum           243     False  0.999988  \n",
       "4         JenniNexus          7788     False  1.000000  \n",
       "5       RoanokeMaven         11622     False  1.000000  \n",
       "6     FromAshestoNew          5981      True  1.000000  \n",
       "7       Lee_G_Malone          9931     False  1.000000  \n",
       "8    UncappingBadger           666     False  1.000000  \n",
       "9            samzorz         16467     False  1.000000  \n",
       "10      bunnyXablaze         17950     False  1.000000  \n",
       "11        Tentanman_          4762     False  1.000000  \n",
       "12        lowcarbyum         27213     False  1.000000  \n",
       "13          AzeraW84         10883     False  1.000000  \n",
       "14     KaraCosplayer          1815     False  1.000000  \n",
       "15      noelledelray          2853     False  1.000000  \n",
       "16    Botulism_Sauce          4654     False  1.000000  \n",
       "17       TheSabaiGuy          5409     False  1.000000  \n",
       "18        ChapperzTV         20033     False  1.000000  \n",
       "19         dlmgaming          1242     False  1.000000  \n",
       "20   PostCubicleKyle         13684     False  1.000000  \n",
       "21           ImShavy         44325     False  1.000000  \n",
       "22         VeeFergie         39896     False  1.000000  \n",
       "23   TheHunterWildTV         37653     False  1.000000  \n",
       "24    blackdoggaming           252     False  0.999994  \n",
       "25    itsamyactually         33742     False  1.000000  \n",
       "26       GimmeGoblin          8727     False  1.000000  \n",
       "27     HullDanh10118         16372     False  1.000000  \n",
       "28        Cupkacenom          6551     False  1.000000  \n",
       "29       BrickinNick          5224     False  1.000000  \n",
       "..               ...           ...       ...       ...  \n",
       "545          Dr_PamC          4682     False  1.000000  \n",
       "546          SXRCXRY          1031     False  1.000000  \n",
       "547  WraightDanielle          2501     False  1.000000  \n",
       "548      TheUNOVisit         14412     False  1.000000  \n",
       "549      y3llowChaoz          5027     False  1.000000  \n",
       "550  thirstaeforjeon          1112     False  1.000000  \n",
       "551      jamiejo2005          1751     False  1.000000  \n",
       "552   StephanieHakim         49752     False  1.000000  \n",
       "553           1andee          1868     False  1.000000  \n",
       "554    MattySimmons_          8748     False  1.000000  \n",
       "555   rebeccaeagless          3830     False  1.000000  \n",
       "556   Stephanieee_12          9694     False  1.000000  \n",
       "557        bluesKLUH         11108     False  1.000000  \n",
       "558         MoonWuff         10859     False  1.000000  \n",
       "559          __Leago         34817     False  1.000000  \n",
       "560       elizaaveth          1858     False  1.000000  \n",
       "561         AOK1YOMI         30584     False  1.000000  \n",
       "562      phbisexuals          8842     False  1.000000  \n",
       "563         yaystout           785     False  1.000000  \n",
       "564      Mikeangel__         11395     False  1.000000  \n",
       "565        greghuber          6301     False  1.000000  \n",
       "566         rashi714         41319     False  1.000000  \n",
       "567    GrogeGamingCR          1763     False  1.000000  \n",
       "568    real_tennille         17493     False  1.000000  \n",
       "569    FaithAlexusss          1870     False  1.000000  \n",
       "570     sugarshookie         19947     False  1.000000  \n",
       "571       lexdaqueen          5220     False  1.000000  \n",
       "572       Dianalmc11         67141     False  1.000000  \n",
       "573      ShopatParis          2424     False  1.000000  \n",
       "574    AngelaMagana1          7074      True  1.000000  \n",
       "\n",
       "[2239 rows x 10 columns]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final model\n",
    "model_final = LogisticRegression().fit(input_array, y)\n",
    "predicts = model_final.predict(x_feats_all)\n",
    "prob = model_final.predict_proba(x_feats_all)\n",
    "print model_final.classes_, prob\n",
    "\n",
    "prob_recommend = prob[:,1].tolist()\n",
    "prob_recommend.sort(reverse=True)\n",
    "prob_recommend_top = prob_recommend[:10]\n",
    "prob_recommend = prob[:,1].tolist()\n",
    "\n",
    "prob_series = pd.Series(prob_recommend)\n",
    "input_test['prob'] = prob_series\n",
    "input_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>favourites_cnt</th>\n",
       "      <th>followers_cnt</th>\n",
       "      <th>friends_cnt</th>\n",
       "      <th>id</th>\n",
       "      <th>listed_cnt</th>\n",
       "      <th>name</th>\n",
       "      <th>statuses_cnt</th>\n",
       "      <th>verified</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1602</td>\n",
       "      <td>625</td>\n",
       "      <td>373</td>\n",
       "      <td>856264641722195970</td>\n",
       "      <td>3</td>\n",
       "      <td>punflower</td>\n",
       "      <td>797</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5254</td>\n",
       "      <td>70576</td>\n",
       "      <td>11877</td>\n",
       "      <td>722210815110041600</td>\n",
       "      <td>367</td>\n",
       "      <td>HandmaidsOnHulu</td>\n",
       "      <td>2992</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>12654</td>\n",
       "      <td>2362</td>\n",
       "      <td>1707</td>\n",
       "      <td>361695170</td>\n",
       "      <td>135</td>\n",
       "      <td>JenniNexus</td>\n",
       "      <td>7788</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>32170</td>\n",
       "      <td>3536</td>\n",
       "      <td>2528</td>\n",
       "      <td>725350782497906688</td>\n",
       "      <td>181</td>\n",
       "      <td>RoanokeMaven</td>\n",
       "      <td>11622</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>33396</td>\n",
       "      <td>102131</td>\n",
       "      <td>45311</td>\n",
       "      <td>1584118603</td>\n",
       "      <td>253</td>\n",
       "      <td>FromAshestoNew</td>\n",
       "      <td>5981</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>12851</td>\n",
       "      <td>1374</td>\n",
       "      <td>754</td>\n",
       "      <td>169604573</td>\n",
       "      <td>61</td>\n",
       "      <td>Lee_G_Malone</td>\n",
       "      <td>9931</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>17408</td>\n",
       "      <td>13840</td>\n",
       "      <td>1689</td>\n",
       "      <td>90995027</td>\n",
       "      <td>86</td>\n",
       "      <td>samzorz</td>\n",
       "      <td>16467</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1277</td>\n",
       "      <td>6205</td>\n",
       "      <td>2200</td>\n",
       "      <td>28719244</td>\n",
       "      <td>106</td>\n",
       "      <td>bunnyXablaze</td>\n",
       "      <td>17950</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>6796</td>\n",
       "      <td>4134</td>\n",
       "      <td>931</td>\n",
       "      <td>2265419863</td>\n",
       "      <td>18</td>\n",
       "      <td>Tentanman_</td>\n",
       "      <td>4762</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3144</td>\n",
       "      <td>18371</td>\n",
       "      <td>15220</td>\n",
       "      <td>173631389</td>\n",
       "      <td>427</td>\n",
       "      <td>lowcarbyum</td>\n",
       "      <td>27213</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   description  favourites_cnt  followers_cnt  friends_cnt  \\\n",
       "0          NaN            1602            625          373   \n",
       "1          NaN            5254          70576        11877   \n",
       "4          NaN           12654           2362         1707   \n",
       "5          NaN           32170           3536         2528   \n",
       "6          NaN           33396         102131        45311   \n",
       "7          NaN           12851           1374          754   \n",
       "9          NaN           17408          13840         1689   \n",
       "10         NaN            1277           6205         2200   \n",
       "11         NaN            6796           4134          931   \n",
       "12         NaN            3144          18371        15220   \n",
       "\n",
       "                    id  listed_cnt             name  statuses_cnt  verified  \\\n",
       "0   856264641722195970           3        punflower           797     False   \n",
       "1   722210815110041600         367  HandmaidsOnHulu          2992      True   \n",
       "4            361695170         135       JenniNexus          7788     False   \n",
       "5   725350782497906688         181     RoanokeMaven         11622     False   \n",
       "6           1584118603         253   FromAshestoNew          5981      True   \n",
       "7            169604573          61     Lee_G_Malone          9931     False   \n",
       "9             90995027          86          samzorz         16467     False   \n",
       "10            28719244         106     bunnyXablaze         17950     False   \n",
       "11          2265419863          18       Tentanman_          4762     False   \n",
       "12           173631389         427       lowcarbyum         27213     False   \n",
       "\n",
       "    prob  \n",
       "0    1.0  \n",
       "1    1.0  \n",
       "4    1.0  \n",
       "5    1.0  \n",
       "6    1.0  \n",
       "7    1.0  \n",
       "9    1.0  \n",
       "10   1.0  \n",
       "11   1.0  \n",
       "12   1.0  "
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommmend_list = input_test[input_test['prob'].isin(prob_recommend_top)][:10]\n",
    "recommmend_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
